# Kafka

services:
  broker:
    image: confluentinc/cp-kafka:${DOCKER_KAFKA_IMAGE_TAG}
    container_name: ${DOCKER_KAFKA_BROKER_CONTAINER_NAME}
    ports:
      - ${KAFKA_BROKER_PORT}:${KAFKA_BROKER_PORT}
    environment:
      KAFKA_NODE_ID: 1 # 노드를 식별하는 유일한 ID
      KAFKA_PROCESS_ROLES: 'broker,controller' # 단일 노드가 브로커와 컨트롤러 역할을 모두 수행
      KAFKA_LISTENERS: PLAINTEXT://:${KAFKA_BROKER_PORT},CONTROLLER://:9093 # 리스너 설정: 클라이언트용(9092), 내부 컨트롤러용(9093)
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${KAFKA_BROKER_HOSTNAME}:${KAFKA_BROKER_PORT} # 외부 클라이언트에게 광고되는 주소
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@broker:9093' # 컨트롤러 쿼럼 구성원 목록 (노드ID@호스트:포트)

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      
      KAFKA_CFG_LOG_DIRS: '/tmp/kraft-storage' # KRaft 메타데이터 및 로그 저장 디렉토리

      CLUSTER_ID: P89R380O22R97Q0Q932E
      KAFKA_CFG_CLUSTER_ID: P89R380O22R97Q0Q932E

      KAFKA_NUM_PARTITIONS: ${PARTITION_COUNT} # 파티션 수

      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 # 오프셋 토픽 복제 인자
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1 # 트랜잭션 최소 ISR 수
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1 # 트랜잭션 상태 로그 복제 인자

      KAFKA_TRANSACTION_MAX_TIMEOUT_MS: 900000 # 트랜잭션 타임아웃 설정 (Producer가 트랜잭션을 완료해야 하는 최대 시간) 15분 (기본값)

    volumes:
      - kafka-data:/tmp/kraft-storage
    deploy:
      resources:
        limits:
          cpus: ${BROKER_COMMON_CPU_LIMIT}
          memory: ${BROKER_COMMON_MEMORY_LIMIT}
    healthcheck:
      # 내부 포트가 연결 가능한지 확인
      test: ["CMD-SHELL", "nc -z localhost ${KAFKA_BROKER_PORT}"] 
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 10s

  topic-creator:
    image: confluentinc/cp-kafka:${DOCKER_KAFKA_IMAGE_TAG}
    depends_on:
      broker:
        condition: service_healthy 
    env_file:
      - ../.env
      - ./.env
    environment:
      KAFKA_BROKER_HOSTNAME: ${KAFKA_BROKER_HOSTNAME}
      KAFKA_BROKER_PORT: ${KAFKA_BROKER_PORT}
      PARTITION_COUNT: ${PARTITION_COUNT}
    command: >
      bash -c '
        for i in `seq 1 30`; do
          kafka-topics --bootstrap-server $$KAFKA_BROKER_HOSTNAME:$$KAFKA_BROKER_PORT \
                       --create \
                       --topic $$KAFKA_TOPIC_NAME \
                       --partitions $$PARTITION_COUNT \
                       --replication-factor 1 \
                       --if-not-exists
          if [ $$? -eq 0 ]; then
            echo "Topic $$KAFKA_TOPIC_NAME created successfully."
            exit 0
          fi
          echo "Topic creation failed, waiting 2 seconds and retrying..."
          sleep 2
        done
        echo "Failed to create topic after 30 attempts."
        exit 1
      '

  consumer:
    build: .
    command: bash -c "export CONSUMER_ID=$(hostname | sed 's/.*-//'); exec python consumer.py"
    env_file:
      - ../.env
      - ./.env
    environment:
      DELIVERY_MODE: ${DELIVERY_MODE} 
      KAFKA_CONSUMER_ISOLATION_LEVEL: ${KAFKA_CONSUMER_ISOLATION_LEVEL:-read_uncommitted}
    volumes:
      - ./consumer.py:/app/consumer.py
      - ./results/consumer:/app/results
    depends_on:
      broker:
        condition: service_healthy
      topic-creator:
        condition: service_completed_successfully

  producer:
    build: .
    command: bash -c "export PRODUCER_ID=$(hostname | sed 's/.*-//'); exec python producer.py"
    env_file:
      - ../.env
      - ./.env
    environment:
      DELIVERY_MODE: ${DELIVERY_MODE}
      KAFKA_RETRIES: ${KAFKA_RETRIES}
      KAFKA_ACKS: ${KAFKA_ACKS}
      KAFKA_ENABLE_IDEMPOTENCE: ${KAFKA_ENABLE_IDEMPOTENCE}
      KAFKA_PRODUCER_TRANSACTIONAL_ID: ${KAFKA_PRODUCER_TRANSACTIONAL_ID:-}
    volumes:
      - ./producer.py:/app/producer.py
      - ./results/producer:/app/results
    depends_on:
      consumer:
        condition: service_started
      broker:
        condition: service_healthy
      topic-creator:
        condition: service_completed_successfully

volumes:
  kafka-data:
